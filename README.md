
# ğŸ§  Offensive Darija Detection

*A Django-based web application for detecting offensive or toxic language in Darija (Moroccan Arabic dialect) using BERT.*

---

## ğŸ“˜ Overview

**Offensive Darija Detection** is an AI-powered web platform that combines **natural language processing** and **content moderation** within a blog system.
Built on **Django** and leveraging a **BERT model fine-tuned for Darija**, it automatically identifies toxic or abusive expressions in user-generated content â€” helping moderators keep discussions safe and respectful.

---

## ğŸš€ Key Features

* ğŸ“ **Blog Platform** â€” Users can register, create, and publish articles.
* ğŸ’¬ **Comment Moderation** â€” Every comment is automatically analyzed for offensiveness. Toxic comments are flagged before publication.
* ğŸ¤– **AI Toxicity Detection** â€” Integrates the **[`Abdelaaziz/toxic-darija-bert-classification`](https://huggingface.co/Abdelaaziz/toxic-darija-bert-classification)** model from HuggingFace.
* ğŸ”’ **Admin Control Panel** â€” Admins can review, approve, or delete flagged comments and manage articles.
* ğŸ–¼ï¸ **Media & UI** â€” Supports article images and provides a responsive **Bootstrap-based interface**.

---

## ğŸ§© Tech Stack

| Layer           | Technology                                     |
| :-------------- | :--------------------------------------------- |
| **Backend**     | Django (Python)                                |
| **AI Model**    | HuggingFace BERT for Darija Toxicity Detection |
| **Database**    | SQLite (default)                               |
| **Frontend**    | HTML â€¢ Bootstrap â€¢ CSS                         |
| **Integration** | HuggingFace API via `requests`                 |
| **Environment** | `.env` configuration for API keys and secrets  |

---

## ğŸ—ï¸ Project Structure

```
OffensiveDarijaDetection/
â”‚
â”œâ”€â”€ login/                # Authentication, blog, and comment logic
â”œâ”€â”€ media/                # Uploaded article images
â”œâ”€â”€ projet_blog/          # Main Django project settings
â”œâ”€â”€ static/               # CSS, JS, and image files
â”œâ”€â”€ templates/            # HTML templates and UI pages
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ toxic_filter.py   # HuggingFace API integration and toxicity logic
â”‚
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ How It Works

1. Users publish an article and post comments.
2. Each comment is sent to the **HuggingFace API**, which returns a **toxicity score**.
3. If the score exceeds a threshold, the comment is **flagged** and hidden pending admin approval.
4. Administrators can **approve, edit, or delete** flagged content directly from the Django admin panel.

---

## ğŸ§  Model Reference

* **Model Name:** [`Abdelaaziz/toxic-darija-bert-classification`](https://huggingface.co/Abdelaaziz/toxic-darija-bert-classification)
* **Architecture:** BERT-based text classification
* **Goal:** Detect and classify toxic or offensive expressions in Moroccan Arabic (Darija).

---

## ğŸ§° Installation & Setup

### 1. Clone the repository

```bash
git clone https://github.com/your-username/offensive-darija-detection.git
cd offensive-darija-detection
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment variables

Create a `.env` file in the project root and add your HuggingFace API key:

```
HUGGINGFACE_API_KEY=your_api_key_here
```

### 4. Apply migrations

```bash
python manage.py migrate
```

### 5. Run the development server

```bash
python manage.py runserver
```

Then visit:
ğŸ‘‰ [http://localhost:8000](http://localhost:8000)

---

## ğŸ§‘â€ğŸ’» Example Use Case

> **Scenario:** A user posts a comment containing a potentially offensive phrase in Darija.
>
> * The app automatically sends the comment text to the BERT model.
> * If itâ€™s detected as toxic, the comment is flagged and hidden.
> * The admin receives a notification and decides whether to approve or delete it.

---

## ğŸ“ˆ Future Improvements

* Add multilingual support (Arabic, French).
* Implement real-time moderation with WebSockets.
* Include visual analytics dashboards for toxicity reports.
* Expand dataset and fine-tune model for better accuracy.

---

## ğŸ§‘â€ğŸ¤â€ğŸ§‘ Contributors

| Name                  | Role                       |
| --------------------- | -------------------------- |
| **Elhoucine Lachgar** | Developer / AI Integration |
| **[Your Teammates]**  | Backend & UI Design        |

---

## ğŸ“œ License

This project is distributed under the **MIT License**.

